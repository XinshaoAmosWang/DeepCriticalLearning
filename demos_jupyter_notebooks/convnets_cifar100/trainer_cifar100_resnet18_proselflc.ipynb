{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas\n",
    "import torch\n",
    "\n",
    "from proselflc.trainer.trainer_cnn_vision_derivedgrad import Trainer\n",
    "\n",
    "import pprint\n",
    "print = pprint.PrettyPrinter(indent=4).pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add params configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'batch_accumu_steps': 10,\n",
      "    'batch_size': 128,\n",
      "    'classes_per_batch': 64,\n",
      "    'counter': 'iteration',\n",
      "    'data_name': 'cifar100',\n",
      "    'device': 'gpu',\n",
      "    'eval_interval': 500,\n",
      "    'exp_base': 16,\n",
      "    'gamma': 0.1,\n",
      "    'logit_soften_T': 0.5,\n",
      "    'loss_mode': 'cross entropy',\n",
      "    'loss_name': 'proselflc',\n",
      "    'lr': 0.2,\n",
      "    'lr_scheduler': 'WarmupMultiStepSchedule',\n",
      "    'milestones': [20000, 30000],\n",
      "    'momentum': 0.9,\n",
      "    'network_name': 'resnet18',\n",
      "    'num_classes': 100,\n",
      "    'num_workers': 8,\n",
      "    'sampler': 'BalancedBatchSampler',\n",
      "    'symmetric_noise_rate': 0.4,\n",
      "    'total_epochs': 100,\n",
      "    'transit_time_ratio': 0.5,\n",
      "    'trust_mode': 'global*(1-H(p)/H(u))',\n",
      "    'warmup_epochs': 0,\n",
      "    'weight_decay': 0.002}\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params.update(\n",
    "    {\n",
    "        \"data_name\": \"cifar100\",\n",
    "        \"num_classes\": 100,  # 1000\n",
    "        \"device\": \"gpu\",\n",
    "        #\n",
    "        \"num_workers\": 8,\n",
    "        #\n",
    "        \"counter\": \"iteration\",\n",
    "        \"classes_per_batch\": 8,\n",
    "        #\n",
    "        # To get deterministic results, please uncomment the line below\n",
    "        # to set seed parameter.\n",
    "        # \"seed\": 123,\n",
    "    }\n",
    ")\n",
    "\n",
    "# data\n",
    "params[\"symmetric_noise_rate\"] = 0.4\n",
    "\n",
    "# network\n",
    "params[\"network_name\"] = \"resnet18\"\n",
    "\n",
    "# for the demo purpose, I set the total epochs to be small.\n",
    "params[\"total_epochs\"] = 100\n",
    "params[\"eval_interval\"] = 500 # iterations\n",
    "\n",
    "# batch\n",
    "params[\"batch_size\"] = 128\n",
    "params[\"sampler\"] = \"BalancedBatchSampler\"\n",
    "params[\"classes_per_batch\"] = 64\n",
    "\n",
    "# learning rate\n",
    "params[\"lr\"] = 0.2\n",
    "params[\"weight_decay\"] = 2e-3 # larger weight decay for resnet18\n",
    "params[\"lr_scheduler\"] = \"WarmupMultiStepSchedule\"\n",
    "params[\"warmup_epochs\"] = 0\n",
    "params[\"milestones\"] = [20000, 30000]\n",
    "\n",
    "# optimisation\n",
    "params[\"momentum\"] = 0.9\n",
    "params[\"batch_accumu_steps\"] = 10\n",
    "params[\"gamma\"] = 0.1\n",
    "\n",
    "# loss settings\n",
    "params[\"loss_mode\"] = \"cross entropy\"\n",
    "params[\"trust_mode\"] = \"global*(1-H(p)/H(u))\"\n",
    "params[\"loss_name\"] = \"proselflc\"\n",
    "params[\"transit_time_ratio\"] = 0.50\n",
    "params[\"exp_base\"] = 16\n",
    "params[\"logit_soften_T\"] = 0.5\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the folder to store intermediate and final results\n",
    "* First, run `sudo mkdir /home/proselflc_experiments/ && sudo chmod -R 777 /home/proselflc_experiments/` so that so you have the write permission.\n",
    "* Or set `WORK_DIR = \"/home/your_username/proselflc_experiments/\"`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = \"/home/proselflc_experiments/\"\n",
    "#\n",
    "# use the time as a unique experiment identifier\n",
    "dt_string = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "summary_writer_dir = (\n",
    "    params[\"loss_name\"]\n",
    "    + \"_\"\n",
    "    + dt_string\n",
    ")\n",
    "params[\"summary_writer_dir\"] = (\n",
    "    WORK_DIR\n",
    "    + \"/\"\n",
    "    + params[\"data_name\"]\n",
    "    + \"_symmetric_noise_rate_\"\n",
    "    + str(params[\"symmetric_noise_rate\"])\n",
    "    + \"/\"\n",
    "    + params[\"network_name\"]\n",
    "    + \"/\"\n",
    "    + summary_writer_dir\n",
    ")\n",
    "if not os.path.exists(params[\"summary_writer_dir\"]):\n",
    "    os.makedirs(params[\"summary_writer_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Init the trainer, and store the params configurations\n",
    "For each experiment, we have one unique result folder to store the params configurations and learning curves.\n",
    "Therefore, you can revisit any specific experiment whenever you need without losing any details."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(params=params)\n",
    "# params[\"milestones\"] was a list of integers, we convert it to a string before sinking.\n",
    "params[\"milestones\"] = str(params[\"milestones\"])\n",
    "dataframe = pandas.DataFrame(params, index=[0])\n",
    "dataframe.to_csv(\n",
    "    params[\"summary_writer_dir\"] + \"/params.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    index=False,\n",
    "    sep=\"\\t\",\n",
    "    mode=\"w\",  #\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the trainer and save the final model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Network.....\n",
      "Iteration= (500,2)/(39100, 100), lr=0.2000, batch_mean_epsilon=0.0001, valid batch size=128.0000, batch_mean_gtrust=0.0004, batch_mean_etrust=0.1624\n",
      "clean_test: Loss= 4.1282, Accuracy= 0.0819, Entropy= 0.9285, Max_p= 0.0616,\n",
      "noisy_subset: Loss= 4.8826, Accuracy= 0.0106, Entropy= 0.9404, Max_p= 0.0532,\n",
      "clean_subset: Loss= 4.1465, Accuracy= 0.0855, Entropy= 0.9399, Max_p= 0.0537,\n",
      "cleaned_noisy_subset: Loss= 4.1601, Accuracy= 0.0814, Entropy= 0.9402, Max_p= 0.0534,\n",
      "Evaluating Network.....\n",
      "Iteration= (1000,3)/(39100, 100), lr=0.2000, batch_mean_epsilon=0.0001, valid batch size=128.0000, batch_mean_gtrust=0.0005, batch_mean_etrust=0.2128\n",
      "clean_test: Loss= 3.9089, Accuracy= 0.1116, Entropy= 0.9237, Max_p= 0.0665,\n",
      "noisy_subset: Loss= 4.8363, Accuracy= 0.0104, Entropy= 0.9370, Max_p= 0.0569,\n",
      "clean_subset: Loss= 3.9159, Accuracy= 0.1183, Entropy= 0.9359, Max_p= 0.0579,\n",
      "cleaned_noisy_subset: Loss= 3.9486, Accuracy= 0.1108, Entropy= 0.9372, Max_p= 0.0567,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(\n\u001B[1;32m      3\u001B[0m     trainer\u001B[38;5;241m.\u001B[39mnetwork,\n\u001B[1;32m      4\u001B[0m     params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msummary_writer_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/model.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m )\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe experiment is finished with details sinked in \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m      8\u001B[0m         params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msummary_writer_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      9\u001B[0m     )\n\u001B[1;32m     10\u001B[0m )\n",
      "File \u001B[0;32m~/Dropbox/GeneralCareerDevelop/PersonalBrand/GithubRepositories/ProSelfLC_tpami2release/src/proselflc/trainer/trainer_cnn_vision_derivedgrad.py:258\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;66;03m# #############################\u001B[39;00m\n\u001B[1;32m    256\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    257\u001B[0m         \u001B[38;5;66;03m# train one epoch\u001B[39;00m\n\u001B[0;32m--> 258\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m            \u001B[49m\u001B[43mepoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    260\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraindataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;66;03m# #############################\u001B[39;00m\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msink_csv_figures()\n",
      "File \u001B[0;32m~/Dropbox/GeneralCareerDevelop/PersonalBrand/GithubRepositories/ProSelfLC_tpami2release/src/proselflc/trainer/trainer_cnn_vision_derivedgrad.py:286\u001B[0m, in \u001B[0;36mTrainer.train_one_epoch\u001B[0;34m(self, epoch, dataloader)\u001B[0m\n\u001B[1;32m    284\u001B[0m network_inputs \u001B[38;5;241m=\u001B[39m raw_inputs\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 286\u001B[0m     network_inputs \u001B[38;5;241m=\u001B[39m \u001B[43mnetwork_inputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    287\u001B[0m     labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m    288\u001B[0m \u001B[38;5;66;03m# #############################\u001B[39;00m\n\u001B[1;32m    289\u001B[0m \n\u001B[1;32m    290\u001B[0m \u001B[38;5;66;03m# #############################\u001B[39;00m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;66;03m# forward\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "torch.save(\n",
    "    trainer.network,\n",
    "    params[\"summary_writer_dir\"] + \"/model.pt\",\n",
    ")\n",
    "print(\n",
    "    \"The experiment is finished with details sinked in {}\".format(\n",
    "        params[\"summary_writer_dir\"]\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
